{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e19ead6",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2131f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9743f37b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')\n",
    "\n",
    "print(f\"Train shape: {train_data.shape}\")\n",
    "print(f\"Test shape: {test_data.shape}\")\n",
    "print('')\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8cd088",
   "metadata": {},
   "source": [
    "### Display missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Missing values (train set):')\n",
    "print(train_data.isna().sum())\n",
    "print('')\n",
    "print('Missing values (test set):')\n",
    "print(test_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff58a25",
   "metadata": {},
   "source": [
    "### Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f98825",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train set data types:')\n",
    "print(train_data.dtypes)\n",
    "print('')\n",
    "print('Test set data types:')\n",
    "print(test_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82ae27e",
   "metadata": {},
   "source": [
    "We need to convert some features to the appropriate data types (int64 or float64) for the model to work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a1097",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e739a42",
   "metadata": {},
   "source": [
    "### Transported distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91723c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "counts = train_data['Transported'].value_counts()\n",
    "labels = [f\"Transported ({counts[True]})\", f\"Not Transported ({counts[False]})\"]\n",
    "plt.pie(counts, labels=labels, autopct='%1.1f%%')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1a782",
   "metadata": {},
   "source": [
    "### Expenses distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a77202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_features = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "for i, feature in enumerate(exp_features):\n",
    "    # Left plot\n",
    "    ax=fig.add_subplot(5,2,2*i+1)\n",
    "    sns.histplot(data=train_data, x=feature, axes=ax, bins=30, kde=False, hue='Transported')\n",
    "    ax.set_title(feature)\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9848e6",
   "metadata": {},
   "source": [
    "We notice that most passengers have no expenses. We will create a new feature indicating whether a passenger has expenses or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720227d",
   "metadata": {},
   "source": [
    "### CryoSleep distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=train_data, x='CryoSleep', hue='Transported')\n",
    "plt.title('CryoSleep distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0c413",
   "metadata": {},
   "source": [
    "CryoSleep seems to have an impact on the target variable since most passengers in CryoSleep were transported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a672102",
   "metadata": {},
   "source": [
    "### HomePlanet & Destination distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HomePlanet distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=train_data, x='HomePlanet', hue='Transported')\n",
    "plt.title('HomePlanet distribution')\n",
    "plt.show()\n",
    "\n",
    "# Destination distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=train_data, x='Destination', hue='Transported')\n",
    "plt.title('Destination distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d059cc",
   "metadata": {},
   "source": [
    "HomePlanet & Destination do not seem to have a significant impact on the target variable (especially Destination). However, we will keep them for now as they might interact with other features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f93d6a5",
   "metadata": {},
   "source": [
    "### VIP distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "sns.countplot(data=train_data, x='VIP', hue='Transported')\n",
    "plt.title('VIP distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb74f779",
   "metadata": {},
   "source": [
    "VIP does not seem to be a significant feature. We will drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a99d185",
   "metadata": {},
   "source": [
    "### Qualitative features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fcff79",
   "metadata": {},
   "source": [
    "We can't plot all qualitative features yet because they have too many unique values. We will handle them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qualitative_features = ['PassengerId', 'Name', 'Cabin']\n",
    "\n",
    "print(train_data[qualitative_features].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a29c4c0",
   "metadata": {},
   "source": [
    "- We can extract the group of a passenger from the PassengerId feature. We will create a new feature indicating the group of a passenger.\n",
    "- We can extract the deck, num and side of a passenger from the Cabin feature. We will create three new features indicating the deck, num and side of a passenger.\n",
    "- We can extract the last name of a passenger from the Name feature. We will create a new feature indicating the last name of a passenger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db69c2",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72eca3",
   "metadata": {},
   "source": [
    "### Display Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849afbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values summary for train set\n",
    "print(\"Train Set Missing Values:\")\n",
    "na_train_cols = train_data.columns[train_data.isna().any()].tolist()\n",
    "mv_train = pd.DataFrame(train_data[na_train_cols].isna().sum(), columns=['Number_missing'])\n",
    "mv_train['Percentage_missing'] = np.round(100 * mv_train['Number_missing'] / len(train_data), 2)\n",
    "print(mv_train)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Missing values summary for test set\n",
    "print(\"Test Set Missing Values:\")\n",
    "na_test_cols = test_data.columns[test_data.isna().any()].tolist()\n",
    "mv_test = pd.DataFrame(test_data[na_test_cols].isna().sum(), columns=['Number_missing'])\n",
    "mv_test['Percentage_missing'] = np.round(100 * mv_test['Number_missing'] / len(test_data), 2)\n",
    "print(mv_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6512f7",
   "metadata": {},
   "source": [
    "Only 2% of values are missing. However, almost every feature has missing values. We need to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f79d3",
   "metadata": {},
   "source": [
    "### Missing values per passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['MissingPerPassenger'] = train_data.isna().sum(axis=1)\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.countplot(data=train_data, x=train_data['MissingPerPassenger'], hue='Transported')\n",
    "plt.title('Missing values per passenger')\n",
    "train_data = train_data.drop(columns=['MissingPerPassenger'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719e561",
   "metadata": {},
   "source": [
    "We're only missing around 2% of the values, but around 25% of passengers have at least one missing value.\n",
    "The easiest way to handle them could be to use the media for numerical features and the mode for categorical features.\n",
    "However, we will look at the joint distribution of missing values to see if there are patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a6b43e",
   "metadata": {},
   "source": [
    "### Features creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ec5f1b",
   "metadata": {},
   "source": [
    "#### Compute spendings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spendings(data: pd.DataFrame):\n",
    "    data['TotalSpendings'] = data[exp_features].sum(axis=1)\n",
    "    data['HasNotSpent'] = (data['TotalSpendings'] == 0).astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b5783b",
   "metadata": {},
   "source": [
    "#### Passenger groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_groups(data: pd.DataFrame):\n",
    "    data['Group'] = data['PassengerId'].apply(lambda x: x.split('_')[0]).astype(int)\n",
    "    data['GroupSize'] = data.groupby('Group')['Group'].transform('count')\n",
    "    data['IsAlone'] = (data['GroupSize'] == 1).astype(int)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a95c34",
   "metadata": {},
   "source": [
    "#### Split Cabin features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1595b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cabin(data: pd.DataFrame):\n",
    "    data['Cabin'] = data['Cabin'].fillna('Z/9999/Z')\n",
    "\n",
    "    data['CabinDeck'] = data['Cabin'].apply(lambda x: x.split('/')[0])\n",
    "    data['CabinNum']  = data['Cabin'].apply(lambda x: x.split('/')[1]).astype(int)\n",
    "    data['CabinSide'] = data['Cabin'].apply(lambda x: x.split('/')[2])\n",
    "\n",
    "    data.loc[data['CabinDeck'] == 'Z', 'CabinDeck'] = np.nan\n",
    "    data.loc[data['CabinNum'] == 9999, 'CabinNum'] = np.nan\n",
    "    data.loc[data['CabinSide'] == 'Z', 'CabinSide'] = np.nan\n",
    "\n",
    "    data = data.drop('Cabin', axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9df084",
   "metadata": {},
   "source": [
    "#### Compute family size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f22121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_family_size(data: pd.DataFrame):\n",
    "    data['Name'] = data['Name'].fillna('Unknown Unknown')\n",
    "    data['LastName'] = data['Name'].str.split().str[-1]\n",
    "    data['FamilySize'] = data.groupby('LastName')['LastName'].transform('count')\n",
    "\n",
    "    data.loc[data['LastName'] == 'Unknown', 'LastName'] = np.nan\n",
    "    data.loc[data['FamilySize'] > 100, 'FamilySize'] = np.nan\n",
    "\n",
    "    data = data.drop('Name', axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc336c1c",
   "metadata": {},
   "source": [
    "### Handle missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba5265",
   "metadata": {},
   "source": [
    "#### Fill HomePlanet missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_homeplanet_values(data: pd.DataFrame):\n",
    "    GHP_grouped = data.groupby(['Group', 'HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\n",
    "\n",
    "    # Passengers with missing HomePlanet but whose group has known HomePlanet\n",
    "    GHP_index = data[data['HomePlanet'].isna()][(data[data['HomePlanet'].isna()]['Group']).isin(GHP_grouped.index)].index\n",
    "\n",
    "    data.loc[GHP_index, 'HomePlanet'] = data.iloc[GHP_index, :]['Group'].map(lambda x: GHP_grouped.idxmax(axis=1)[x])\n",
    "\n",
    "    # Decks A, B, C or T are from Europa\n",
    "    data.loc[(data['HomePlanet'].isna()) & (data['CabinDeck'].isin(['A', 'B', 'C', 'T'])), 'HomePlanet'] = 'Europa'\n",
    "    # Deck G is from Earth\n",
    "    data.loc[(data['HomePlanet'].isna()) & (data['CabinDeck'] == 'G'), 'HomePlanet'] = 'Earth'\n",
    "\n",
    "    SHP_grouped = data.groupby(['LastName', 'HomePlanet'])['HomePlanet'].size().unstack().fillna(0)\n",
    "\n",
    "    # Passengers with missing HomePlanet but whose surname has known HomePlanet\n",
    "    SHP_index = data[data['HomePlanet'].isna()][(data[data['HomePlanet'].isna()]['LastName']).isin(SHP_grouped.index)].index\n",
    "\n",
    "    data.loc[SHP_index, 'HomePlanet'] = data.iloc[SHP_index, :]['LastName'].map(lambda x: SHP_grouped.idxmax(axis=1)[x])\n",
    "\n",
    "    # No one from deck D is from Earth\n",
    "    # Fill remaining HomePlanet missing values with Earth (if not on deck D) or Mars (if on Deck D)\n",
    "    data.loc[(data['HomePlanet'].isna()) & ~(data['CabinDeck'] == 'D'), 'HomePlanet'] = 'Earth'\n",
    "    data.loc[(data['HomePlanet'].isna()) & (data['CabinDeck'] == 'D'), 'HomePlanet'] = 'Mars'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f28bf",
   "metadata": {},
   "source": [
    "#### Fill Destination missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e74b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_destination_values(data: pd.DataFrame):\n",
    "    # Most passengers are going to TRAPPIST-1e\n",
    "    data.loc[(data['Destination'].isna()), 'Destination'] = 'TRAPPIST-1e'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b940b1",
   "metadata": {},
   "source": [
    "#### Fill LastName missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23422664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_lastname_values(data: pd.DataFrame):\n",
    "    # Group by Group and LastName if Group size > 1\n",
    "    GLN_grouped = data[data['GroupSize'] > 1].groupby(['Group', 'LastName'])['LastName'].size().unstack().fillna(0)\n",
    "\n",
    "    # Passengers with missing LastName but whose group has known LastName\n",
    "    GLN_index = data[data['LastName'].isna()][(data[data['LastName'].isna()]['Group']).isin(GLN_grouped.index)].index\n",
    "\n",
    "    data.loc[GLN_index, 'LastName'] = data.iloc[GLN_index, :]['Group'].map(lambda x: GLN_grouped.idxmax(axis=1)[x])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d01525",
   "metadata": {},
   "source": [
    "#### Fill family size missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d16a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_family_size_values(data: pd.DataFrame):\n",
    "    data['LastName'] = data['LastName'].fillna('Unknown')\n",
    "\n",
    "    data['FamilySize'] = data['LastName'].map(lambda x: data['LastName'].value_counts()[x])\n",
    "\n",
    "    data.loc[data['LastName'] == 'Unknown', 'FamilySize'] = np.nan\n",
    "    # Unknown last name means no family\n",
    "    data.loc[data['FamilySize'] > 100, 'FamilySize'] = 0\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0fb4c",
   "metadata": {},
   "source": [
    "#### Fill Cabin Side missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186347fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_cabin_side_values(data: pd.DataFrame):\n",
    "    GCS_grouped = data[data['GroupSize'] > 1].groupby(['Group', 'CabinSide'])['CabinSide'].size().unstack().fillna(0)\n",
    "\n",
    "    # Passengers with missing CabinSide but whose group has known CabinSide\n",
    "    GCS_index = data[data['CabinSide'].isna()][(data[data['CabinSide'].isna()]['Group']).isin(GCS_grouped.index)].index\n",
    "\n",
    "    data.loc[GCS_index, 'CabinSide'] = data.iloc[GCS_index, :]['Group'].map(lambda x: GCS_grouped.idxmax(axis=1)[x])\n",
    "\n",
    "    SCS_grouped = data[data['GroupSize'] > 1].groupby(['LastName', 'CabinSide'])['CabinSide'].size().unstack().fillna(0)\n",
    "\n",
    "    # Passengers with missing CabinSide but whose surname has known CabinSide\n",
    "    SCS_index = data[data['CabinSide'].isna()][(data[data['CabinSide'].isna()]['LastName']).isin(SCS_grouped.index)].index\n",
    "\n",
    "    data.loc[SCS_index, 'CabinSide'] = data.iloc[SCS_index, :]['LastName'].map(lambda x: SCS_grouped.idxmax(axis=1)[x])\n",
    "\n",
    "    data = data.drop('LastName', axis=1)\n",
    "\n",
    "    # Fill remaining CabinSide missing values with 'Z' (unknown)\n",
    "    data.loc[data['CabinSide'].isna(), 'CabinSide'] = 'Z'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34960236",
   "metadata": {},
   "source": [
    "#### Fill Cabin Deck missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617e76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_cabin_deck_values(data: pd.DataFrame):\n",
    "    GCD_grouped = data[data['GroupSize'] > 1].groupby(['Group', 'CabinDeck'])['CabinDeck'].size().unstack().fillna(0)\n",
    "\n",
    "    # Passengers with missing CabinDeck but whose group has known CabinDeck\n",
    "    GCD_grouped_index = data[data['CabinDeck'].isna()][(data[data['CabinDeck'].isna()]['Group']).isin(GCD_grouped.index)].index\n",
    "\n",
    "    data.loc[GCD_grouped_index, 'CabinDeck'] = data.iloc[GCD_grouped_index, :]['Group'].map(lambda x: GCD_grouped.idxmax(axis=1)[x])\n",
    "\n",
    "    CD_na_rows = data.loc[data['CabinDeck'].isna(), 'CabinDeck'].index\n",
    "    data.loc[data['CabinDeck'].isna(), 'CabinDeck'] = data.groupby(['HomePlanet', 'Destination', 'IsAlone'])['CabinDeck'].transform(lambda x: x.fillna(pd.Series.mode(x)[0]))[CD_na_rows]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94921108",
   "metadata": {},
   "source": [
    "#### Fill VIP missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f2a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_vip_values(data: pd.DataFrame):\n",
    "    # Most passengers are not VIP\n",
    "    data.loc[data['VIP'].isna(), 'VIP'] = False\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b031414",
   "metadata": {},
   "source": [
    "#### Fill CryoSleep missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_cryosleep_values(data: pd.DataFrame):\n",
    "    # Passengers who did not spend anything are likely in CryoSleep\n",
    "    data.loc[(data['CryoSleep'].isna()) & (data['HasNotSpent'] == 1), 'CryoSleep'] = True\n",
    "    data.loc[(data['CryoSleep'].isna()) & (data['HasNotSpent'] == 0), 'CryoSleep'] = False\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266c5f9",
   "metadata": {},
   "source": [
    "#### Fill Spendings missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff40f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_spendings_values(data: pd.DataFrame):\n",
    "    # Passengers who are in CryoSleep are not expected to have spendings\n",
    "    for feature in exp_features:\n",
    "        data.loc[(data[feature].isna() & (data['CryoSleep'] == True)), feature] = 0\n",
    "        median_value = data.loc[data['CryoSleep'] == False, feature].median()\n",
    "        data.loc[(data[feature].isna() & (data['CryoSleep'] == False)), feature] = median_value\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660937b",
   "metadata": {},
   "source": [
    "#### Clean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80780a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data: pd.DataFrame):\n",
    "    # Create features\n",
    "    compute_spendings(data)\n",
    "    create_groups(data)\n",
    "    split_cabin(data)\n",
    "    compute_family_size(data)\n",
    "\n",
    "    # Fill missing values\n",
    "    fill_homeplanet_values(data)\n",
    "    fill_destination_values(data)\n",
    "    fill_lastname_values(data)\n",
    "    fill_family_size_values(data)\n",
    "    fill_cabin_side_values(data)\n",
    "    fill_cabin_deck_values(data)\n",
    "    fill_vip_values(data)\n",
    "    fill_cryosleep_values(data)\n",
    "    fill_spendings_values(data)\n",
    "\n",
    "    # Drop unneeded features\n",
    "    data = data.drop(['PassengerId', 'Group', 'GroupSize'], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "cleaned_train_data = clean(train_data)\n",
    "# print(cleaned_train_data.head())\n",
    "cleaned_test_data = clean(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa87bf",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9db0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cleaned_train_data['Transported']\n",
    "\n",
    "features = [\n",
    "    'CryoSleep',\n",
    "    'RoomService',\n",
    "    'FoodCourt',\n",
    "    'ShoppingMall',\n",
    "    'Spa',\n",
    "    'VRDeck',\n",
    "    'TotalSpendings',\n",
    "    'HomePlanet',\n",
    "    'RoomService',\n",
    "    'FoodCourt',\n",
    "    'ShoppingMall',\n",
    "    'Spa',\n",
    "    'VRDeck',\n",
    "    'Destination',\n",
    "    'Age',\n",
    "    'CabinDeck',\n",
    "    'FamilySize',\n",
    "    'IsAlone',\n",
    "    'CabinSide',\n",
    "    'HasNotSpent',\n",
    "    'VIP',\n",
    "]\n",
    "\n",
    "X = pd.get_dummies(cleaned_train_data[features])\n",
    "X_test = pd.get_dummies(cleaned_test_data[features])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "output = pd.DataFrame({ 'PassengerId': test_data.PassengerId, 'Transported': predictions })\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f86142",
   "metadata": {},
   "source": [
    "## Model Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # Kaggle Score Simulation\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(\"KAGGLE SCORE SIMULATION\")\n",
    "# print(\"=\"*50)\n",
    "\n",
    "# # Separate features and target\n",
    "# y = train_data['Transported']\n",
    "# X = train_data.drop('Transported', axis=1)\n",
    "\n",
    "# # Clean the data\n",
    "# X_cleaned = clean(X.copy())\n",
    "\n",
    "# # Handle remaining missing values and encode categorical variables\n",
    "# X_cleaned = X_cleaned.fillna(X_cleaned.median(numeric_only=True))\n",
    "# X_cleaned = pd.get_dummies(X_cleaned, drop_first=True)\n",
    "\n",
    "# # Cross-validation\n",
    "# model = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=42)\n",
    "# cv_scores = cross_val_score(model, X_cleaned, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# print(f\"\\nCross-Validation Scores: {cv_scores}\")\n",
    "# print(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "# print(f\"\\n📊 Expected Kaggle Score Range: {cv_scores.mean() - cv_scores.std():.4f} - {cv_scores.mean() + cv_scores.std():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
